{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUWHsLugWBTwzOLh1cFvBX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/workjuanmejia/MaestriaIA/blob/main/Project_Cancer_Uterino.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/workjuanmejia/MaestriaIA.git\n",
        "%cd MaestriaIA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKigT5n9-2IQ",
        "outputId": "c7623f71-578f-4969-f11b-33b79ab96d86"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MaestriaIA'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (6/6), done.\n",
            "/content/MaestriaIA/MaestriaIA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8INz6yNbrVb6"
      },
      "outputs": [],
      "source": [
        "!pip -q install requests pandas python-slugify\n",
        "\n",
        "import os, math, time, requests, pandas as pd\n",
        "from urllib.parse import urlparse\n",
        "from slugify import slugify\n",
        "\n",
        "# ----- Opcional: token de Socrata para mayores cuotas/velocidad -----\n",
        "# Coloca tu token si lo tienes (Settings > Environment en Colab o aquí directamente)\n",
        "SOCRATA_APP_TOKEN = os.getenv(\"SOCRATA_APP_TOKEN\", \"\")  # ej. \"abcd1234...\"\n",
        "\n",
        "# ======== HELPERS ========\n",
        "\n",
        "def socrata_to_csv(domain: str, dataset_id: str, out_name: str, limit=50000, sleep_s=0.8, select=None, where=None):\n",
        "    \"\"\"\n",
        "    Descarga TODO el dataset Socrata (SODA) con paginación y lo guarda en CSV.\n",
        "    domain: 'www.datos.gov.co'\n",
        "    dataset_id: como 'jba4-yke'\n",
        "    out_name: nombre de archivo CSV a guardar.\n",
        "    \"\"\"\n",
        "    base = f\"https://{domain}/resource/{dataset_id}.json\"\n",
        "    headers = {\"X-App-Token\": SOCRATA_APP_TOKEN} if SOCRATA_APP_TOKEN else {}\n",
        "    offset = 0\n",
        "    frames = []\n",
        "    while True:\n",
        "        params = {\"$limit\": limit, \"$offset\": offset}\n",
        "        if select: params[\"$select\"] = select\n",
        "        if where:  params[\"$where\"] = where\n",
        "\n",
        "        r = requests.get(base, headers=headers, params=params, timeout=60)\n",
        "        r.raise_for_status()\n",
        "        chunk = r.json()\n",
        "        if not chunk:\n",
        "            break\n",
        "        frames.append(pd.DataFrame(chunk))\n",
        "        offset += limit\n",
        "        # cortesía para no saturar\n",
        "        time.sleep(sleep_s)\n",
        "\n",
        "    if not frames:\n",
        "        print(f\"[Socrata] Sin datos: {domain}/{dataset_id}\")\n",
        "        return\n",
        "\n",
        "    df = pd.concat(frames, ignore_index=True)\n",
        "    # Normaliza nombres\n",
        "    df.columns = [slugify(c, separator=\"_\") for c in df.columns]\n",
        "    df.to_csv(out_name, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"✅ [Socrata] Guardado {out_name} | filas: {len(df)} | cols: {len(df.columns)}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def ckan_dataset_slug_to_resource_ids(domain: str, dataset_slug: str):\n",
        "    \"\"\"\n",
        "    Dado un dataset SLUG en CKAN (p.ej. 'morbilidad-cancer-de-cuello-uterino'),\n",
        "    trae los resources (UUIDs) usando package_show.\n",
        "    \"\"\"\n",
        "    url = f\"https://{domain}/api/3/action/package_show\"\n",
        "    r = requests.get(url, params={\"id\": dataset_slug}, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    pkg = r.json()\n",
        "    if not pkg.get(\"success\"):\n",
        "        raise RuntimeError(f\"CKAN package_show falló: {pkg}\")\n",
        "    resources = pkg[\"result\"].get(\"resources\", [])\n",
        "    # Filtrar solo recursos con datastore activo (tabulares)\n",
        "    return [res for res in resources if res.get(\"datastore_active\")]\n",
        "\n",
        "\n",
        "def ckan_resource_to_csv(domain: str, resource_id: str, out_name: str, limit=50000, sleep_s=0.8, fields=None, filters=None):\n",
        "    \"\"\"\n",
        "    Descarga TODO el resource CKAN mediante datastore_search con paginación.\n",
        "    \"\"\"\n",
        "    base = f\"https://{domain}/api/3/action/datastore_search\"\n",
        "    offset = 0\n",
        "    frames = []\n",
        "    while True:\n",
        "        params = {\"resource_id\": resource_id, \"limit\": limit, \"offset\": offset}\n",
        "        if fields:  params[\"fields\"]  = fields\n",
        "        if filters: params[\"filters\"] = filters  # JSON string si lo usas\n",
        "\n",
        "        r = requests.get(base, params=params, timeout=120)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        if not data.get(\"success\"):\n",
        "            raise RuntimeError(f\"CKAN datastore_search falló: {data}\")\n",
        "        records = data[\"result\"][\"records\"]\n",
        "        if not records:\n",
        "            break\n",
        "        frames.append(pd.DataFrame.from_records(records))\n",
        "        offset += limit\n",
        "        time.sleep(sleep_s)\n",
        "\n",
        "    if not frames:\n",
        "        print(f\"[CKAN] Sin datos: {domain} | resource {resource_id}\")\n",
        "        return\n",
        "\n",
        "    df = pd.concat(frames, ignore_index=True)\n",
        "    df.columns = [slugify(c, separator=\"_\") for c in df.columns]\n",
        "    df.to_csv(out_name, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"✅ [CKAN] Guardado {out_name} | filas: {len(df)} | cols: {len(df.columns)}\")\n",
        "    return df\n"
      ]
    }
  ]
}