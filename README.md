# ðŸ“Š PIPELINE: Datos de cÃ¡ncer de cuello uterino en Cali y BogotÃ¡

En este proyecto se aplicÃ³ un proceso de **ExtracciÃ³n, TransformaciÃ³n y Carga (ETL)** a bases de datos de salud sobre cÃ¡ncer de cuello uterino en BogotÃ¡ y Cali, con el fin de analizar inequidades en los **tiempos de atenciÃ³n oportuna**.  

- **BogotÃ¡**: mÃ¡s de **35.000 registros**.  
- **Cali**: muestra de **883 registros**.  

En la **transformaciÃ³n** se estandarizaron fechas, categorÃ­as de rÃ©gimen y estrato socioeconÃ³mico, ademÃ¡s de crear la variable **tiempo de espera entre examen y diagnÃ³stico**.  
En la **carga**, se consolidaron las bases depuradas para su anÃ¡lisis.  

ðŸ”Ž **Resultados principales**:
- En **BogotÃ¡**, el tiempo promedio de espera fue **12 dÃ­as**.  
- En **Cali**, ascendiÃ³ a **76 dÃ­as**, con mayor variabilidad.  
- Se observaron diferencias segÃºn **estrato socioeconÃ³mico, edad y rÃ©gimen de afiliaciÃ³n**.  

Este proyecto demuestra cÃ³mo un pipeline **ETL asegura la calidad de los datos** y permite obtener evidencias sÃ³lidas para comprender desigualdades en la salud pÃºblica.

---

El pipeline se estructurÃ³ de la siguiente manera:
![Blank diagram (1)](https://github.com/user-attachments/assets/5e5530bc-2496-4652-867d-e1741a79f712)

1. **ExtracciÃ³n** â†’ Datos abiertos en formato CSV.  
2. **TransformaciÃ³n** â†’ Limpieza, normalizaciÃ³n de variables y creaciÃ³n de indicadores.  
3. **Almacenamiento** â†’ BigQuery (Google Cloud).  
4. **VisualizaciÃ³n** â†’ Power BI.  

## CÃ³digo Carga de Datos

La secciÃ³n de datos de Cali y BogotÃ¡ corresponde a la **extracciÃ³n** desde los archivos CSV.  
En este paso:  
- Se convierten todos los campos a **minÃºsculas**.  
- Se eliminan espacios en blanco.  
- Se estandarizan los valores de **rÃ©gimen**: `subsidiado`, `contributivo`, `no_afiliado`, `excepcion_especial`.
- Para poder replicar el resultado se debe especificar la ruta correcta de los archivos CSV 

```python
FILE_CALI = "/content/drive/MyDrive/Estadistica/43.-cancer-de-cuello-uterino-d.abiertos-piii-2023-2022-2021.csv"

# Lectura del archivo CSV
df_cl = pd.read_csv(FILE_CALI, sep=",", low_memory=False)

# NormalizaciÃ³n de texto
for col in df_cl.select_dtypes(include="object").columns:
    df_cl[col] = df_cl[col].str.lower().str.strip()

# EstandarizaciÃ³n de categorÃ­as
df_cl["tip_ss_"] = df_cl["tip_ss_"].replace(mapa_tip_ss)

print(f"[Cali] Filas: {len(df_cl):,} | Columnas: {df_cl.shape[1]}")
```
## Consolidacion de campos de los Dataframes

- En esta seccion se definiÃ³ un formato para estandarizar el formato de la fecha en las dos fuentes de datos.
- Se selecionaron los siguientes parametros que se van a analizar para los datos de ambas Ciudades: `Ciudad`, `AÃ±o`, `Estrato`, `Regimen`, `Etnia`, `Fecha de examen`, `Fecha de diagnostico`, `Territorio`, `FEdad`, `Sexo`.
  
```python
#BogotÃ¡
bog = pd.DataFrame()
bog["ciudad"]    = df_bg['ciudad']
bog["ano"]      = df_bg["ano"]
bog["estrato"]   = df_bg["estrato_"]
bog["regimen"]   = df_bg["tip_ss_"]
bog["etnia"]     = df_bg["nom_grupo_"]
bog["fecha_examen"] = df_bg["fec_toma_e"].apply(parse_fecha_espanol)
bog["fecha_diag"]   = df_bg["fec_res_ex"].apply(parse_fecha_espanol)
bog["territorio"]   = df_bg["LOCALIDAD_RESIDENCIA"]
bog["edad"]         = pd.to_numeric(df_bg["EDAD_"], errors="coerce")
bog["sexo"]         = df_bg["SEXO"]

#Cali
cll = pd.DataFrame()
cll["ciudad"]    = df_cl["ciudad"]
cll["ano"]      = parse_date(df_cl["fec_toma_e"]).dt.year
cll["estrato"]   = df_cl["codigo_sspm"]   # No estÃ¡ en este dataset
cll["regimen"]   = df_cl["tip_ss_"]
cll["etnia"]     = df_cl["per_etn_"]
# Convert to string before applying the parsing function
cll["fecha_examen"] = df_cl["fec_toma_e"].astype(str).apply(parse_fecha_es)
cll["fecha_diag"]   = df_cl["fec_con_"].astype(str).apply(parse_fecha_es)
cll["territorio"]   = df_cl["ciudad"]   # solo ciudad, no comuna
cll["edad"]         = pd.to_numeric(df_cl["edad_"], errors="coerce")
cll["sexo"]         = df_cl["sexo_"]

```

## Agregar columna de tiempo de espera en dÃ­as

- Se agregÃ³ una columna calculada a cada uno de los dataframes que calcula la cantidad de dÃ­as que tomÃ³ el diagnostico de cada paciente (fecha diagnostico - Fecha de examen)
  
```python
#Tiempo de espera en dÃ­as Cali
cll["tiempo_espera_dias"] = (cll["fecha_diag"] - cll["fecha_examen"]).dt.days

#Filtro datos de Cali por filas donde esten las fechas de diagnostico y el examen
cll_filtrado = cll[(cll["fecha_examen"].notna()) &
              (cll["fecha_diag"].notna()) &
              (cll["fecha_diag"] >= cll["fecha_examen"])].copy()

#Tiempo de espera en dÃ­as BogotÃ¡
bog["tiempo_espera_dias"] = (bog["fecha_diag"] - bog["fecha_examen"]).dt.days

#Filtro datos de Cali por filas donde esten las fechas de diagnostico y el examen
bog_filtrado = bog[(bog["fecha_examen"].notna()) &
              (bog["fecha_diag"].notna()) &
              (bog["fecha_diag"] >= bog["fecha_examen"])].copy()
```

## Carga de Datos a big query

Primero se debe establecer la informacion de conexion basica a la base de datos. (Nombre del proyecto, dataset, Id de la tabla donde se va a cargar la informacion)
```python
from google.cloud import bigquery
from google.colab import auth

auth.authenticate_user()
project_id = "analisis-ccu"
dataset_id = "Datos_CCU"      

client = bigquery.Client(project=project_id)

dataset_ref = bigquery.Dataset(f"{project_id}.{dataset_id}")
dataset_ref.location = "southamerica-east1"  #se escogio esta region porque es la mas cercana a colombia
client.create_dataset(dataset_ref, exists_ok=True)

table_id =  f"{project_id}.{dataset_id}.Fact_data_CCU"

# 2) Esquema datos
schema = [
    bigquery.SchemaField("ciudad", "STRING"),
    bigquery.SchemaField("ano", "INTEGER"),
    bigquery.SchemaField("estrato", "INTEGER"),
    bigquery.SchemaField("regimen", "STRING"),
    bigquery.SchemaField("etnia", "STRING"),
    bigquery.SchemaField("fecha_examen", "DATE"),
    bigquery.SchemaField("fecha_diag", "DATE"),
    bigquery.SchemaField("territorio", "STRING"),
    bigquery.SchemaField("edad", "INTEGER"),
    bigquery.SchemaField("sexo", "STRING"),
]

job_config = bigquery.LoadJobConfig(
    schema=schema,
    write_disposition="WRITE_TRUNCATE",  # usa WRITE_APPEND si quieres anexar
)

job = client.load_table_from_dataframe(data_unificada, table_id, job_config=job_config)
job.result()
print("df unificadso cargado en:", table_id_bog)

```

